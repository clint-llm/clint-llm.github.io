Validity and reliability in assessments.

The intended purpose of an assessment is to determine whether an assessment tool gives valid and reliable results that are wholly attributable to the learner. An assessment tool has to give reproducible results, and after many trials, statistical analyses can determine areas with a variation. This determines the effectiveness of the assessment tool. If there is a previously established ideal, against which others are compared, then a novel assessment should correlate the results to this; however, many times, there is no such "gold standard," and thus, a comparison should be made to similar assessment tools.

Reliability relates to the uniformity of a measure.

- The Kuder-Richardson coefficient for two-answer questions and Cronbach's alpha for questions with more than two answers can be used to measure internal consistency where 2 to 3 questions are generated that measure the same concept, and the correlation among the answers is measured. Strong correlations where the reliability estimate is as close to 1 as possible (higher than 0.7), indicate high reliability, while weak correlations indicate the assessment tool may not be reliable.

- Test-retest reliability is measured when an assessment tool is given to the same learners more than once, at different times, and under similar conditions. The correlation between the measurements at different time points is then calculated. Similarly, parallel-form or alternate-form reliability is similar to test-retest except that a different form of the original assessment is given to learners in the following evaluations. For example, the concepts being tested are the same in both versions of the assessment, but the phrasing of it is different.

- Interrater reliability is used to study the effect of different assessors using the same assessment tools. Consistency in rater scores relates to the level of inter-rater reliability of the assessment tool. It is estimated by Cohen's Kappa, which compares the proportion of actual agreement between raters to the proportion expected to agree by chance.

- Analysis of variance (ANOVA) is another tool to generate a generalizability coefficient. This theory recognizes that multiple sources of error and true score variance exist and that measures may have different reliabilities in different situations. This method aims to quantify how much measurement error is attributable to each potential factor, such as differences in question phrasing, learner attributes, raters, or time between assessments. This model looks at the overall reliability of the results.

The validity of an assessment tool refers to how well the tool measures what it intends to measure. High reliability is not the only measure of efficacy for an assessment tool; other measures of validity are necessary to determine the integrity of the assessment approach. Determining validity requires evidence to support the use of the assessment tool in a particular context. The development of new tools is not always necessary, but they must be appropriate for program activities, and reliability and validity reported or references cited for each assessment tool used.

- Evidence for content validity is the "relationship between a test's content and the construct it is intended to measure." This refers to the themes, wording, and format of the items presented in an assessment tool.

- The response process involves the analyses of the responses to the assessment and includes the strategies and thought processes of individual learners. Analyzing the variance in response patterns between different types of learners may reveal sources of inconsistency, and that is irrelevant to the concept being measured.

- The internal structure of the assessment tool refers to "the degree to which the relationships among test items and test components conform to the construct on which the proposed test score interpretations are based." Evidence to support the internal structure of an assessment includes dimensionality, measurement invariance, and reliability.

- Evidence for the relation to other variables involves the statistical relationship between assessment scores and another measure relevant to the measured construct. A strongly positive relationship would indicate two measures that measure the same construct, or a negligible relationship would describe measures that should be independent.

- Consequences refer to effects from the administration of the assessment tool. In other words, consequences evidence assesses the impact, whether positive or negative and intended or unintended, of the assessment itself.